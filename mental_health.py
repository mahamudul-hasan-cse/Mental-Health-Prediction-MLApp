# -*- coding: utf-8 -*-
"""mental_health.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yzxPR5eWPih_L76DityvnadBuwzvN2_B
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
import warnings

warnings.filterwarnings('ignore') # Suppress warnings for cleaner output

from google.colab import files
uploaded = files.upload()  # Upload the file using file picker

import pandas as pd
df = pd.read_csv("mental_health_dataset.csv")  # Use exact uploaded filename

df.head()

df.info()

df.describe()

df.describe(include=['object', 'category'])

missing_values = df.isnull().sum()
missing_values

for col in df.columns:
    unique_vals = df[col].unique()
    print(f"{col}: {df[col].nunique()} unique values (Top 5: {unique_vals[:5]})")
target_variable = 'mental_health_risk'

plt.figure(figsize=(8, 6))
sns.countplot(x=target_variable, data=df, order=df[target_variable].value_counts().index)
plt.title(f'Distribution of Target Variable: {target_variable}')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

X_features_df = df.drop(target_variable, axis=1, errors='ignore')

numerical_features = X_features_df.select_dtypes(include=np.number).columns.tolist()
categorical_features = X_features_df.select_dtypes(include=['object', 'category']).columns.tolist()

df[numerical_features].hist(bins=15, figsize=(15, max(10, len(numerical_features) * 1.5)), layout=(-1, 3))
plt.suptitle('Histograms of Numerical Features', y=1.02)
plt.tight_layout()
plt.show()

# Boxplots of Numerical Features


plt.figure(figsize=(15, max(5, len(numerical_features) * 1.5)))

for i, col in enumerate(numerical_features):

    plt.subplot((len(numerical_features) + 2) // 3, 3, i + 1)

    sns.boxplot(y=df[col])

    plt.title(col)


plt.suptitle('Boxplots of Numerical Features')
plt.tight_layout()

plt.show()

plt.figure(figsize=(15, max(6, len(numerical_features) * 2)))

for i, col in enumerate(numerical_features):
    plt.subplot((len(numerical_features) + 2) // 3, 3, i + 1)

    sns.boxplot(x=df[target_variable], y=df[col], order=sorted(df[target_variable].unique().tolist()))

    plt.title(f'{col} vs {target_variable}')
    plt.xticks(rotation=45, ha='right')

plt.suptitle(f'Boxplots of Numerical Features vs {target_variable}', y=1.02)
plt.tight_layout()  # Adjust layout to fit suptitle
plt.show()

plt.figure(figsize=(18, max(6, len(categorical_features) * 2.5)))

for i, col in enumerate(categorical_features):
    plt.subplot((len(categorical_features) + 1) // 2, 2, i + 1)

    # Show top 15 categories for readability
    order = df[col].value_counts().index[:15] if df[col].nunique() > 15 else df[col].value_counts().index
    sns.countplot(y=df[col], order=order)

    plt.title(f'Distribution of {col} (Top 15 if >15 cats)')

plt.suptitle('Countplots of Categorical Features', y=1.02)
plt.tight_layout()
plt.show()

plt.figure(figsize=(18, max(7, len(categorical_features) * 3)))

for i, col in enumerate(categorical_features):
    plt.subplot((len(categorical_features) + 1) // 2, 2, i + 1)

    y_order = df[col].value_counts().index[:10] if df[col].nunique() > 10 else df[col].value_counts().index
    hue_order = sorted(df[target_variable].unique().tolist())

    sns.countplot(y=df[col], hue=df[target_variable], order=y_order, hue_order=hue_order)
    plt.title(f'{col} vs {target_variable} (Y-axis: Top 10 if >10 cats)')

plt.suptitle(f'Countplots of Categorical Features vs {target_variable}', y=1.02)
plt.tight_layout(rect=[0, 0, 1, 0.97])
plt.show()

plt.figure(figsize=(max(10, len(numerical_features) * 0.8), max(8, len(numerical_features) * 0.6)))

temp_df_for_corr = df[numerical_features].copy()

le_corr = LabelEncoder()
if df[target_variable].dtype == 'object' or pd.api.types.is_categorical_dtype(df[target_variable]):
    temp_df_for_corr[target_variable] = le_corr.fit_transform(df[target_variable])
else:
    temp_df_for_corr[target_variable] = df[target_variable]

correlation_matrix = temp_df_for_corr.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5, annot_kws={"size": 8})
plt.title('Correlation Heatmap (Numerical Features + Encoded Target)')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

df_processed = df.copy()

for col in numerical_features:
    median_val = df_processed[col].median()
    df_processed[col].fillna(median_val, inplace=True)

for col in categorical_features:
    mode_val = df_processed[col].mode()[0]
    df_processed[col].fillna(mode_val, inplace=True)

df_processed.dropna(subset=[target_variable], inplace=True)

y_series = df_processed[target_variable]
le_target = LabelEncoder()
y = le_target.fit_transform(y_series)

X = df_processed[numerical_features + categorical_features]

high_cardinality_threshold = 15
high_cardinality_cols = [col for col in categorical_features if X[col].nunique() > high_cardinality_threshold]

final_numerical_features = [col for col in numerical_features if col in X.columns]
final_categorical_features = [col for col in categorical_features if col in X.columns]

transformers_list = []


transformers_list.append(('num', StandardScaler(), final_numerical_features))


transformers_list.append(('cat', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False), final_categorical_features))

preprocessor = ColumnTransformer(
    transformers=transformers_list,
    remainder='passthrough'
)

value_counts_y = pd.Series(y).value_counts()

stratify_option = y


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=stratify_option)

models = {
    "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000, solver='liblinear', class_weight='balanced' if stratify_option is not None else None),
    "Random Forest": RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced' if stratify_option is not None else None),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42, n_estimators=100)
}

results = {}
trained_pipelines = {}

for model_name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', model)])
    pipeline.fit(X_train, y_train)
    trained_pipelines[model_name] = pipeline

    y_pred_train = pipeline.predict(X_train)
    y_pred_test = pipeline.predict(X_test)

    accuracy_train = accuracy_score(y_train, y_pred_train)
    accuracy_test = accuracy_score(y_test, y_pred_test)

    print(f"\n{model_name} Performance:")
    print(f"Training Accuracy: {accuracy_train:.4f}")
    print(f"Test Accuracy: {accuracy_test:.4f}")

    print("\nClassification Report (Test Set):")
    class_names = le_target.classes_ if 'le_target' in locals() else None
    print(classification_report(y_test, y_pred_test, target_names=class_names, zero_division=0))

    print("\nConfusion Matrix (Test Set):")
    cm = confusion_matrix(y_test, y_pred_test)
    plt.figure(figsize=(max(6, len(le_target.classes_)*1.5), max(4, len(le_target.classes_)*1)))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names if class_names is not None else 'auto',
                yticklabels=class_names if class_names is not None else 'auto')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

roc_auc = roc_auc_score(y_test, pipeline.predict_proba(X_test), multi_class='ovr')

results[model_name] = {
    'Training Accuracy': accuracy_train,
    'Test Accuracy': accuracy_test,
    'Classification Report': classification_report(y_test, y_pred_test, target_names=class_names, output_dict=True, zero_division=0),
    'Confusion Matrix': cm,
    'ROC AUC': roc_auc
}

valid_results_items = [item for item in results.items() if 'Error' not in item[1] and 'Test Accuracy' in item[1]]
sorted_results = sorted(valid_results_items, key=lambda item: item[1].get('Test Accuracy', 0), reverse=True)

for model_name, metrics in sorted_results:
    roc_auc_val = metrics.get('ROC AUC', 'N/A')
    roc_auc_str = f"{roc_auc_val:.4f}" if isinstance(roc_auc_val, float) else str(roc_auc_val)
    print(f"{model_name}: Test Accuracy = {metrics.get('Test Accuracy', 0):.4f}, ROC AUC = {roc_auc_str}")

best_model_name = None
best_model_name = sorted_results[0][0]
best_accuracy = sorted_results[0][1].get('Test Accuracy', 0)
print(f"\nBest model based on Test Accuracy: {best_model_name} with accuracy {best_accuracy:.4f}")

from sklearn.metrics import RocCurveDisplay, roc_curve, auc
from sklearn.preprocessing import label_binarize

plt.figure(figsize=(12, 10))
ax = plt.gca()

# Binarize the target variable for OvR plotting
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
n_classes = y_test_bin.shape[1]

for model_name, pipeline in trained_pipelines.items():
    if hasattr(pipeline, 'predict_proba'):
        y_score = pipeline.predict_proba(X_test)
        # Plot ROC curve for each class (OvR)
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
            roc_auc = auc(fpr, tpr)
            ax.plot(fpr, tpr, label=f'{model_name} (Class {le_target.classes_[i]} - AUC = {roc_auc:.2f})')
    else:
        print(f"Model {model_name} does not have predict_proba method.")


plt.title('ROC Curve Comparison (One-vs-Rest)')
plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')  # Add random chance line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()